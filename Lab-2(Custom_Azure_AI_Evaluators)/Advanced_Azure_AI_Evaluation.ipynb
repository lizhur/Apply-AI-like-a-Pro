{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/lizhur/Apply-AI-like-a-Pro/blob/main/Lab-2(Custom_Azure_AI_Evaluators)/Advanced_Azure_AI_Evaluation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rei_fV8bttWq"
      },
      "source": [
        "# Azure AI Evaluation\n",
        "\n",
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](<https://colab.research.google.com/github/sachin0034/hands_on_AI_introduction_to_AI_evaluations-4038348/blob/main/Lab-3%28Building_AI_evaluators%29/Advanced_Azure_AI_Evaluation.ipynb>)\n",
        "\n",
        "\n",
        "In this lab, we will learn about **Azure AI Evaluation**, which refers to a set of tools and services provided by Microsoft Azure designed to assess and monitor the performance, safety, and quality of artificial intelligence applications—particularly generative AI and machine learning models.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HD1cPgrIt4Zz"
      },
      "source": [
        "# Get Started"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "70avKV8luGW5"
      },
      "source": [
        "## Prerequisites\n",
        "\n",
        "Before starting the lab, ensure you have the following resources in place for a smooth and successful experience:\n",
        "\n",
        "---\n",
        "\n",
        "### Azure Subscription Requirement\n",
        "\n",
        "> **Important:**  \n",
        "> This lab requires an **Azure account with a Premium (Pay-as-you-go or Enterprise) subscription**.  \n",
        "> Free-tier or trial accounts **do not** provide access to Azure OpenAI services.\n",
        "\n",
        "---\n",
        "\n",
        "### 1. Sample Contract File for Testing\n",
        "\n",
        "You’ll need a sample contract file to test the application.\n",
        "\n",
        "- **Download the sample contract here:**  \n",
        "  [Download Sample Contract (Google Drive)](https://drive.google.com/file/d/1E557kdNBZ5cDUvVDLNrEVRuKcRSYDG3Z/view?usp=sharing)\n",
        "\n",
        "---\n",
        "\n",
        "### 2. OpenAI API Key\n",
        "\n",
        "An OpenAI API key is required to access the language models used for contract evaluation.\n",
        "\n",
        "- **Don’t have an OpenAI API key? Follow this step-by-step guide:**  \n",
        "  [How to get your own OpenAI API key (Medium article)](https://medium.com/@lorenzozar/how-to-get-your-own-openai-api-key-f4d44e60c327)\n",
        "\n",
        "---\n",
        "\n",
        "### 3. Foundry Project Setup\n",
        "\n",
        "As part of the setup, you are required to create a Foundry project.\n",
        "\n",
        "- **Follow the instructions here:**  \n",
        "  [Create a Foundry Project](../Create_Azure_Project_Instruction/Readme.md)\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kV7pOTlXvR_i"
      },
      "source": [
        "# Step 1: Install Dependencies and Their Descriptions\n",
        "\n",
        "To begin working with Azure AI Evaluation and related tools, install all required Python packages\n",
        "\n",
        "\n",
        "```| Package Name             | Description                                                                                         |\n",
        "|-------------------------|-----------------------------------------------------------------------------------------------------|\n",
        "| **langchain**           | Framework to build applications powered by language models, helping with prompt management and chaining AI responses.\n",
        "| **pypdf**               | Library to read and manipulate PDF files, useful for processing contract documents in PDF format.     \n",
        "| **docx2txt**            | Simple tool to extract text from Microsoft Word (.docx) files, enabling text extraction from contracts.\n",
        "| **pandas**              | Data analysis and manipulation library, useful for handling structured evaluation results and datasets.\n",
        "| **openai**              | Python client for OpenAI API, allowing access to OpenAI language models for text generation and evaluation.\n",
        "| **gradio**              | Easy-to-use library to build interactive UIs for machine learning demos, helpful for building evaluation interfaces.\n",
        "| **azure-ai-generative** | Azure SDK to use Azure’s generative AI capabilities including chat and text generation.                 \n",
        "| **langchain-community** | Community-maintained extensions and tools for LangChain to support additional AI workflow features.   \n",
        "| **azure-ai-evaluation** | Azure AI Evaluation SDK providing built-in evaluators to measure AI-generated content quality and safety.\n",
        "| **azure-ai-projects**   | SDK to manage Azure AI Foundry projects and execute evaluations, enabling integration with Azure's AI management tools.\n",
        "| **semantic-kernel**     | Framework for building AI apps with semantic memory and prompt orchestration, enhancing complex AI workflows.\n",
        "\n",
        "```\n",
        "\n",
        "\n",
        "Installing these packages equips your Azure AI contract evaluation environment with capabilities for document processing, language model access, interactive interfaces, and quality evaluation tools.\n",
        "\n",
        "---\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "p2iyXfvAjAZH",
        "outputId": "f5c57da3-7ff0-4c01-f27f-fc2b66f384a4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: langchain in /usr/local/lib/python3.12/dist-packages (0.3.27)\n",
            "Collecting pypdf\n",
            "  Downloading pypdf-6.1.1-py3-none-any.whl.metadata (7.1 kB)\n",
            "Collecting docx2txt\n",
            "  Downloading docx2txt-0.9-py3-none-any.whl.metadata (529 bytes)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (2.2.2)\n",
            "Requirement already satisfied: openai in /usr/local/lib/python3.12/dist-packages (1.108.0)\n",
            "Requirement already satisfied: gradio in /usr/local/lib/python3.12/dist-packages (5.46.0)\n",
            "Collecting azure-ai-generative\n",
            "  Downloading azure_ai_generative-1.0.0b11-py3-none-any.whl.metadata (5.1 kB)\n",
            "Collecting langchain-community\n",
            "  Downloading langchain_community-0.3.30-py3-none-any.whl.metadata (3.0 kB)\n",
            "Collecting azure-ai-evaluation\n",
            "  Downloading azure_ai_evaluation-1.11.1-py3-none-any.whl.metadata (45 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.3/45.3 kB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting azure-ai-projects\n",
            "  Downloading azure_ai_projects-1.0.0-py3-none-any.whl.metadata (21 kB)\n",
            "Collecting semantic-kernel\n",
            "  Downloading semantic_kernel-1.37.0-py3-none-any.whl.metadata (12 kB)\n",
            "Requirement already satisfied: langchain-core<1.0.0,>=0.3.72 in /usr/local/lib/python3.12/dist-packages (from langchain) (0.3.76)\n",
            "Requirement already satisfied: langchain-text-splitters<1.0.0,>=0.3.9 in /usr/local/lib/python3.12/dist-packages (from langchain) (0.3.11)\n",
            "Requirement already satisfied: langsmith>=0.1.17 in /usr/local/lib/python3.12/dist-packages (from langchain) (0.4.28)\n",
            "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in /usr/local/lib/python3.12/dist-packages (from langchain) (2.11.9)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.12/dist-packages (from langchain) (2.0.43)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.12/dist-packages (from langchain) (2.32.4)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.12/dist-packages (from langchain) (6.0.2)\n",
            "Requirement already satisfied: numpy>=1.26.0 in /usr/local/lib/python3.12/dist-packages (from pandas) (2.0.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.12/dist-packages (from openai) (4.10.0)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.12/dist-packages (from openai) (1.9.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from openai) (0.28.1)\n",
            "Requirement already satisfied: jiter<1,>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from openai) (0.11.0)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.12/dist-packages (from openai) (1.3.1)\n",
            "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.12/dist-packages (from openai) (4.67.1)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.11 in /usr/local/lib/python3.12/dist-packages (from openai) (4.15.0)\n",
            "Requirement already satisfied: aiofiles<25.0,>=22.0 in /usr/local/lib/python3.12/dist-packages (from gradio) (24.1.0)\n",
            "Requirement already satisfied: brotli>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from gradio) (1.1.0)\n",
            "Requirement already satisfied: fastapi<1.0,>=0.115.2 in /usr/local/lib/python3.12/dist-packages (from gradio) (0.116.2)\n",
            "Requirement already satisfied: ffmpy in /usr/local/lib/python3.12/dist-packages (from gradio) (0.6.1)\n",
            "Requirement already satisfied: gradio-client==1.13.0 in /usr/local/lib/python3.12/dist-packages (from gradio) (1.13.0)\n",
            "Requirement already satisfied: groovy~=0.1 in /usr/local/lib/python3.12/dist-packages (from gradio) (0.1.2)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.33.5 in /usr/local/lib/python3.12/dist-packages (from gradio) (0.35.0)\n",
            "Requirement already satisfied: jinja2<4.0 in /usr/local/lib/python3.12/dist-packages (from gradio) (3.1.6)\n",
            "Requirement already satisfied: markupsafe<4.0,>=2.0 in /usr/local/lib/python3.12/dist-packages (from gradio) (3.0.2)\n",
            "Requirement already satisfied: orjson~=3.0 in /usr/local/lib/python3.12/dist-packages (from gradio) (3.11.3)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from gradio) (25.0)\n",
            "Requirement already satisfied: pillow<12.0,>=8.0 in /usr/local/lib/python3.12/dist-packages (from gradio) (11.3.0)\n",
            "Requirement already satisfied: pydub in /usr/local/lib/python3.12/dist-packages (from gradio) (0.25.1)\n",
            "Requirement already satisfied: python-multipart>=0.0.18 in /usr/local/lib/python3.12/dist-packages (from gradio) (0.0.20)\n",
            "Requirement already satisfied: ruff>=0.9.3 in /usr/local/lib/python3.12/dist-packages (from gradio) (0.13.0)\n",
            "Requirement already satisfied: safehttpx<0.2.0,>=0.1.6 in /usr/local/lib/python3.12/dist-packages (from gradio) (0.1.6)\n",
            "Requirement already satisfied: semantic-version~=2.0 in /usr/local/lib/python3.12/dist-packages (from gradio) (2.10.0)\n",
            "Requirement already satisfied: starlette<1.0,>=0.40.0 in /usr/local/lib/python3.12/dist-packages (from gradio) (0.48.0)\n",
            "Requirement already satisfied: tomlkit<0.14.0,>=0.12.0 in /usr/local/lib/python3.12/dist-packages (from gradio) (0.13.3)\n",
            "Requirement already satisfied: typer<1.0,>=0.12 in /usr/local/lib/python3.12/dist-packages (from gradio) (0.17.4)\n",
            "Requirement already satisfied: uvicorn>=0.14.0 in /usr/local/lib/python3.12/dist-packages (from gradio) (0.35.0)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.12/dist-packages (from gradio-client==1.13.0->gradio) (2025.3.0)\n",
            "Requirement already satisfied: websockets<16.0,>=10.0 in /usr/local/lib/python3.12/dist-packages (from gradio-client==1.13.0->gradio) (15.0.1)\n",
            "Collecting azure-ai-resources>=1.0.0b7 (from azure-ai-generative)\n",
            "  Downloading azure_ai_resources-1.0.0b9-py3-none-any.whl.metadata (2.7 kB)\n",
            "Collecting mlflow-skinny<3,>=1.27.0 (from azure-ai-generative)\n",
            "  Downloading mlflow_skinny-2.22.2-py3-none-any.whl.metadata (31 kB)\n",
            "Collecting opencensus-ext-azure~=1.0 (from azure-ai-generative)\n",
            "  Downloading opencensus_ext_azure-1.1.15-py2.py3-none-any.whl.metadata (17 kB)\n",
            "Collecting opencensus-ext-logging<=0.1.1 (from azure-ai-generative)\n",
            "  Downloading opencensus_ext_logging-0.1.1-py2.py3-none-any.whl.metadata (2.3 kB)\n",
            "Collecting requests<3,>=2 (from langchain)\n",
            "  Downloading requests-2.32.5-py3-none-any.whl.metadata (4.9 kB)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.12/dist-packages (from langchain-community) (3.12.15)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in /usr/local/lib/python3.12/dist-packages (from langchain-community) (8.5.0)\n",
            "Collecting dataclasses-json<0.7.0,>=0.6.7 (from langchain-community)\n",
            "  Downloading dataclasses_json-0.6.7-py3-none-any.whl.metadata (25 kB)\n",
            "Requirement already satisfied: pydantic-settings<3.0.0,>=2.10.1 in /usr/local/lib/python3.12/dist-packages (from langchain-community) (2.10.1)\n",
            "Requirement already satisfied: httpx-sse<1.0.0,>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from langchain-community) (0.4.1)\n",
            "Requirement already satisfied: pyjwt>=2.8.0 in /usr/local/lib/python3.12/dist-packages (from azure-ai-evaluation) (2.10.1)\n",
            "Collecting azure-identity>=1.16.0 (from azure-ai-evaluation)\n",
            "  Downloading azure_identity-1.25.0-py3-none-any.whl.metadata (87 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m88.0/88.0 kB\u001b[0m \u001b[31m8.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting azure-core>=1.30.2 (from azure-ai-evaluation)\n",
            "  Downloading azure_core-1.35.1-py3-none-any.whl.metadata (46 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.5/46.5 kB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: nltk>=3.9.1 in /usr/local/lib/python3.12/dist-packages (from azure-ai-evaluation) (3.9.1)\n",
            "Collecting azure-storage-blob>=12.10.0 (from azure-ai-evaluation)\n",
            "  Downloading azure_storage_blob-12.26.0-py3-none-any.whl.metadata (26 kB)\n",
            "Collecting ruamel.yaml<1.0.0,>=0.17.10 (from azure-ai-evaluation)\n",
            "  Downloading ruamel.yaml-0.18.15-py3-none-any.whl.metadata (25 kB)\n",
            "Collecting msrest>=0.6.21 (from azure-ai-evaluation)\n",
            "  Downloading msrest-0.7.1-py3-none-any.whl.metadata (21 kB)\n",
            "Collecting isodate>=0.6.1 (from azure-ai-projects)\n",
            "  Downloading isodate-0.7.2-py3-none-any.whl.metadata (11 kB)\n",
            "Collecting azure-ai-agents>=1.0.0 (from azure-ai-projects)\n",
            "  Downloading azure_ai_agents-1.1.0-py3-none-any.whl.metadata (52 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m52.8/52.8 kB\u001b[0m \u001b[31m5.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting azure-ai-projects\n",
            "  Downloading azure_ai_projects-1.1.0b4-py3-none-any.whl.metadata (24 kB)\n",
            "Collecting azure-ai-agents>=1.0.0 (from azure-ai-projects)\n",
            "  Downloading azure_ai_agents-1.2.0b5-py3-none-any.whl.metadata (74 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m74.7/74.7 kB\u001b[0m \u001b[31m7.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting cloudevents~=1.0 (from semantic-kernel)\n",
            "  Downloading cloudevents-1.12.0-py3-none-any.whl.metadata (7.2 kB)\n",
            "Requirement already satisfied: defusedxml~=0.7 in /usr/local/lib/python3.12/dist-packages (from semantic-kernel) (0.7.1)\n",
            "Collecting openapi_core<0.20,>=0.18 (from semantic-kernel)\n",
            "  Downloading openapi_core-0.19.5-py3-none-any.whl.metadata (6.6 kB)\n",
            "Collecting aiortc>=1.9.0 (from semantic-kernel)\n",
            "  Downloading aiortc-1.13.0-py3-none-any.whl.metadata (4.9 kB)\n",
            "Requirement already satisfied: opentelemetry-api~=1.24 in /usr/local/lib/python3.12/dist-packages (from semantic-kernel) (1.37.0)\n",
            "Requirement already satisfied: opentelemetry-sdk~=1.24 in /usr/local/lib/python3.12/dist-packages (from semantic-kernel) (1.37.0)\n",
            "Collecting prance<25.4.9,>=23.6.21 (from semantic-kernel)\n",
            "  Downloading prance-25.4.8.0-py3-none-any.whl.metadata (13 kB)\n",
            "Collecting pybars4~=0.9 (from semantic-kernel)\n",
            "  Downloading pybars4-0.9.13.tar.gz (29 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: nest-asyncio~=1.6 in /usr/local/lib/python3.12/dist-packages (from semantic-kernel) (1.6.0)\n",
            "Requirement already satisfied: scipy>=1.15.1 in /usr/local/lib/python3.12/dist-packages (from semantic-kernel) (1.16.2)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.12/dist-packages (from semantic-kernel) (5.29.5)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.4.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.7.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (6.6.4)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (0.3.2)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.20.1)\n",
            "Collecting aioice<1.0.0,>=0.10.1 (from aiortc>=1.9.0->semantic-kernel)\n",
            "  Downloading aioice-0.10.1-py3-none-any.whl.metadata (4.1 kB)\n",
            "Collecting av<15.0.0,>=14.0.0 (from aiortc>=1.9.0->semantic-kernel)\n",
            "  Downloading av-14.4.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.6 kB)\n",
            "Requirement already satisfied: cffi>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from aiortc>=1.9.0->semantic-kernel) (2.0.0)\n",
            "Collecting cryptography>=44.0.0 (from aiortc>=1.9.0->semantic-kernel)\n",
            "  Downloading cryptography-46.0.2-cp311-abi3-manylinux_2_34_x86_64.whl.metadata (5.7 kB)\n",
            "Requirement already satisfied: google-crc32c>=1.1 in /usr/local/lib/python3.12/dist-packages (from aiortc>=1.9.0->semantic-kernel) (1.7.1)\n",
            "Collecting pyee>=13.0.0 (from aiortc>=1.9.0->semantic-kernel)\n",
            "  Downloading pyee-13.0.0-py3-none-any.whl.metadata (2.9 kB)\n",
            "Collecting pylibsrtp>=0.10.0 (from aiortc>=1.9.0->semantic-kernel)\n",
            "  Downloading pylibsrtp-0.12.0-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.1 kB)\n",
            "Collecting pyopenssl>=25.0.0 (from aiortc>=1.9.0->semantic-kernel)\n",
            "  Downloading pyopenssl-25.3.0-py3-none-any.whl.metadata (17 kB)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.12/dist-packages (from anyio<5,>=3.5.0->openai) (3.10)\n",
            "Collecting azure-ai-ml>=1.14.0 (from azure-ai-resources>=1.0.0b7->azure-ai-generative)\n",
            "  Downloading azure_ai_ml-1.29.0-py3-none-any.whl.metadata (40 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m40.1/40.1 kB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting azure-mgmt-resource<23.0.0,>=22.0.0 (from azure-ai-resources>=1.0.0b7->azure-ai-generative)\n",
            "  Downloading azure_mgmt_resource-22.0.0-py3-none-any.whl.metadata (30 kB)\n",
            "Requirement already satisfied: six>=1.11.0 in /usr/local/lib/python3.12/dist-packages (from azure-core>=1.30.2->azure-ai-evaluation) (1.17.0)\n",
            "Collecting msal>=1.30.0 (from azure-identity>=1.16.0->azure-ai-evaluation)\n",
            "  Downloading msal-1.34.0-py3-none-any.whl.metadata (11 kB)\n",
            "Collecting msal-extensions>=1.2.0 (from azure-identity>=1.16.0->azure-ai-evaluation)\n",
            "  Downloading msal_extensions-1.3.1-py3-none-any.whl.metadata (7.8 kB)\n",
            "Collecting deprecation<3.0,>=2.0 (from cloudevents~=1.0->semantic-kernel)\n",
            "  Downloading deprecation-2.1.0-py2.py3-none-any.whl.metadata (4.6 kB)\n",
            "Collecting marshmallow<4.0.0,>=3.18.0 (from dataclasses-json<0.7.0,>=0.6.7->langchain-community)\n",
            "  Downloading marshmallow-3.26.1-py3-none-any.whl.metadata (7.3 kB)\n",
            "Collecting typing-inspect<1,>=0.4.0 (from dataclasses-json<0.7.0,>=0.6.7->langchain-community)\n",
            "  Downloading typing_inspect-0.9.0-py3-none-any.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->openai) (2025.8.3)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->openai) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.16.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.33.5->gradio) (3.19.1)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.33.5->gradio) (1.1.10)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.12/dist-packages (from langchain-core<1.0.0,>=0.3.72->langchain) (1.33)\n",
            "Requirement already satisfied: requests-toolbelt>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from langsmith>=0.1.17->langchain) (1.0.0)\n",
            "Requirement already satisfied: zstandard>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from langsmith>=0.1.17->langchain) (0.25.0)\n",
            "Requirement already satisfied: cachetools<6,>=5.0.0 in /usr/local/lib/python3.12/dist-packages (from mlflow-skinny<3,>=1.27.0->azure-ai-generative) (5.5.2)\n",
            "Requirement already satisfied: click<9,>=7.0 in /usr/local/lib/python3.12/dist-packages (from mlflow-skinny<3,>=1.27.0->azure-ai-generative) (8.2.1)\n",
            "Requirement already satisfied: cloudpickle<4 in /usr/local/lib/python3.12/dist-packages (from mlflow-skinny<3,>=1.27.0->azure-ai-generative) (3.1.1)\n",
            "Collecting databricks-sdk<1,>=0.20.0 (from mlflow-skinny<3,>=1.27.0->azure-ai-generative)\n",
            "  Downloading databricks_sdk-0.67.0-py3-none-any.whl.metadata (39 kB)\n",
            "Requirement already satisfied: gitpython<4,>=3.1.9 in /usr/local/lib/python3.12/dist-packages (from mlflow-skinny<3,>=1.27.0->azure-ai-generative) (3.1.45)\n",
            "Requirement already satisfied: importlib_metadata!=4.7.0,<9,>=3.7.0 in /usr/local/lib/python3.12/dist-packages (from mlflow-skinny<3,>=1.27.0->azure-ai-generative) (8.7.0)\n",
            "Collecting packaging (from gradio)\n",
            "  Downloading packaging-24.2-py3-none-any.whl.metadata (3.2 kB)\n",
            "Requirement already satisfied: sqlparse<1,>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from mlflow-skinny<3,>=1.27.0->azure-ai-generative) (0.5.3)\n",
            "Requirement already satisfied: requests-oauthlib>=0.5.0 in /usr/local/lib/python3.12/dist-packages (from msrest>=0.6.21->azure-ai-evaluation) (2.0.0)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.12/dist-packages (from nltk>=3.9.1->azure-ai-evaluation) (1.5.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.12/dist-packages (from nltk>=3.9.1->azure-ai-evaluation) (2024.11.6)\n",
            "Requirement already satisfied: jsonschema<5.0.0,>=4.18.0 in /usr/local/lib/python3.12/dist-packages (from openapi_core<0.20,>=0.18->semantic-kernel) (4.25.1)\n",
            "Collecting jsonschema-path<0.4.0,>=0.3.1 (from openapi_core<0.20,>=0.18->semantic-kernel)\n",
            "  Downloading jsonschema_path-0.3.4-py3-none-any.whl.metadata (4.3 kB)\n",
            "Requirement already satisfied: more-itertools in /usr/local/lib/python3.12/dist-packages (from openapi_core<0.20,>=0.18->semantic-kernel) (10.8.0)\n",
            "Collecting openapi-schema-validator<0.7.0,>=0.6.0 (from openapi_core<0.20,>=0.18->semantic-kernel)\n",
            "  Downloading openapi_schema_validator-0.6.3-py3-none-any.whl.metadata (5.4 kB)\n",
            "Collecting openapi-spec-validator<0.8.0,>=0.7.1 (from openapi_core<0.20,>=0.18->semantic-kernel)\n",
            "  Downloading openapi_spec_validator-0.7.2-py3-none-any.whl.metadata (5.7 kB)\n",
            "Collecting parse (from openapi_core<0.20,>=0.18->semantic-kernel)\n",
            "  Downloading parse-1.20.2-py2.py3-none-any.whl.metadata (22 kB)\n",
            "Collecting werkzeug<3.1.2 (from openapi_core<0.20,>=0.18->semantic-kernel)\n",
            "  Downloading werkzeug-3.1.1-py3-none-any.whl.metadata (3.7 kB)\n",
            "Collecting opencensus<1.0.0,>=0.11.4 (from opencensus-ext-azure~=1.0->azure-ai-generative)\n",
            "  Downloading opencensus-0.11.4-py2.py3-none-any.whl.metadata (12 kB)\n",
            "Requirement already satisfied: psutil>=5.6.3 in /usr/local/lib/python3.12/dist-packages (from opencensus-ext-azure~=1.0->azure-ai-generative) (5.9.5)\n",
            "Requirement already satisfied: opentelemetry-semantic-conventions==0.58b0 in /usr/local/lib/python3.12/dist-packages (from opentelemetry-sdk~=1.24->semantic-kernel) (0.58b0)\n",
            "Requirement already satisfied: chardet>=5.2 in /usr/local/lib/python3.12/dist-packages (from prance<25.4.9,>=23.6.21->semantic-kernel) (5.2.0)\n",
            "Collecting PyMeta3>=0.5.1 (from pybars4~=0.9->semantic-kernel)\n",
            "  Downloading PyMeta3-0.5.1.tar.gz (29 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.12/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.4.1)\n",
            "Requirement already satisfied: python-dotenv>=0.21.0 in /usr/local/lib/python3.12/dist-packages (from pydantic-settings<3.0.0,>=2.10.1->langchain-community) (1.1.1)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2->langchain) (3.4.3)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2->langchain) (2.5.0)\n",
            "Collecting ruamel.yaml.clib>=0.2.7 (from ruamel.yaml<1.0.0,>=0.17.10->azure-ai-evaluation)\n",
            "  Downloading ruamel.yaml.clib-0.2.14-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.0 kB)\n",
            "Requirement already satisfied: greenlet>=1 in /usr/local/lib/python3.12/dist-packages (from SQLAlchemy<3,>=1.4->langchain) (3.2.4)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.12/dist-packages (from typer<1.0,>=0.12->gradio) (1.5.4)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.12/dist-packages (from typer<1.0,>=0.12->gradio) (13.9.4)\n",
            "Collecting dnspython>=2.0.0 (from aioice<1.0.0,>=0.10.1->aiortc>=1.9.0->semantic-kernel)\n",
            "  Downloading dnspython-2.8.0-py3-none-any.whl.metadata (5.7 kB)\n",
            "Collecting ifaddr>=0.2.0 (from aioice<1.0.0,>=0.10.1->aiortc>=1.9.0->semantic-kernel)\n",
            "  Downloading ifaddr-0.2.0-py3-none-any.whl.metadata (4.9 kB)\n",
            "Collecting azure-mgmt-core>=1.3.0 (from azure-ai-ml>=1.14.0->azure-ai-resources>=1.0.0b7->azure-ai-generative)\n",
            "  Downloading azure_mgmt_core-1.6.0-py3-none-any.whl.metadata (4.6 kB)\n",
            "Collecting strictyaml<2.0.0 (from azure-ai-ml>=1.14.0->azure-ai-resources>=1.0.0b7->azure-ai-generative)\n",
            "  Downloading strictyaml-1.7.3-py3-none-any.whl.metadata (11 kB)\n",
            "Collecting colorama<1.0.0 (from azure-ai-ml>=1.14.0->azure-ai-resources>=1.0.0b7->azure-ai-generative)\n",
            "  Downloading colorama-0.4.6-py2.py3-none-any.whl.metadata (17 kB)\n",
            "Collecting azure-storage-file-share (from azure-ai-ml>=1.14.0->azure-ai-resources>=1.0.0b7->azure-ai-generative)\n",
            "  Downloading azure_storage_file_share-12.22.0-py3-none-any.whl.metadata (50 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.9/50.9 kB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting azure-storage-file-datalake>=12.2.0 (from azure-ai-ml>=1.14.0->azure-ai-resources>=1.0.0b7->azure-ai-generative)\n",
            "  Downloading azure_storage_file_datalake-12.21.0-py3-none-any.whl.metadata (16 kB)\n",
            "Collecting pydash<9.0.0,>=6.0.0 (from azure-ai-ml>=1.14.0->azure-ai-resources>=1.0.0b7->azure-ai-generative)\n",
            "  Downloading pydash-8.0.5-py3-none-any.whl.metadata (4.5 kB)\n",
            "Collecting azure-common>=1.1 (from azure-ai-ml>=1.14.0->azure-ai-resources>=1.0.0b7->azure-ai-generative)\n",
            "  Downloading azure_common-1.1.28-py2.py3-none-any.whl.metadata (5.0 kB)\n",
            "Collecting azure-monitor-opentelemetry (from azure-ai-ml>=1.14.0->azure-ai-resources>=1.0.0b7->azure-ai-generative)\n",
            "  Downloading azure_monitor_opentelemetry-1.8.1-py3-none-any.whl.metadata (23 kB)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.12/dist-packages (from cffi>=1.0.0->aiortc>=1.9.0->semantic-kernel) (2.23)\n",
            "Requirement already satisfied: google-auth~=2.0 in /usr/local/lib/python3.12/dist-packages (from databricks-sdk<1,>=0.20.0->mlflow-skinny<3,>=1.27.0->azure-ai-generative) (2.38.0)\n",
            "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.12/dist-packages (from gitpython<4,>=3.1.9->mlflow-skinny<3,>=1.27.0->azure-ai-generative) (4.0.12)\n",
            "Requirement already satisfied: zipp>=3.20 in /usr/local/lib/python3.12/dist-packages (from importlib_metadata!=4.7.0,<9,>=3.7.0->mlflow-skinny<3,>=1.27.0->azure-ai-generative) (3.23.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.12/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<1.0.0,>=0.3.72->langchain) (3.0.0)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.12/dist-packages (from jsonschema<5.0.0,>=4.18.0->openapi_core<0.20,>=0.18->semantic-kernel) (2025.9.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.12/dist-packages (from jsonschema<5.0.0,>=4.18.0->openapi_core<0.20,>=0.18->semantic-kernel) (0.36.2)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.12/dist-packages (from jsonschema<5.0.0,>=4.18.0->openapi_core<0.20,>=0.18->semantic-kernel) (0.27.1)\n",
            "Collecting pathable<0.5.0,>=0.4.1 (from jsonschema-path<0.4.0,>=0.3.1->openapi_core<0.20,>=0.18->semantic-kernel)\n",
            "  Downloading pathable-0.4.4-py3-none-any.whl.metadata (1.8 kB)\n",
            "Requirement already satisfied: rfc3339-validator in /usr/local/lib/python3.12/dist-packages (from openapi-schema-validator<0.7.0,>=0.6.0->openapi_core<0.20,>=0.18->semantic-kernel) (0.1.4)\n",
            "Collecting lazy-object-proxy<2.0.0,>=1.7.1 (from openapi-spec-validator<0.8.0,>=0.7.1->openapi_core<0.20,>=0.18->semantic-kernel)\n",
            "  Downloading lazy_object_proxy-1.12.0-cp312-cp312-manylinux1_x86_64.manylinux_2_28_x86_64.manylinux_2_5_x86_64.whl.metadata (5.1 kB)\n",
            "Collecting opencensus-context>=0.1.3 (from opencensus<1.0.0,>=0.11.4->opencensus-ext-azure~=1.0->azure-ai-generative)\n",
            "  Downloading opencensus_context-0.1.3-py2.py3-none-any.whl.metadata (3.3 kB)\n",
            "Requirement already satisfied: google-api-core<3.0.0,>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from opencensus<1.0.0,>=0.11.4->opencensus-ext-azure~=1.0->azure-ai-generative) (2.25.1)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.12/dist-packages (from requests-oauthlib>=0.5.0->msrest>=0.6.21->azure-ai-evaluation) (3.3.1)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.12/dist-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio) (4.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.12/dist-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio) (2.19.2)\n",
            "Collecting mypy-extensions>=0.3.0 (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7.0,>=0.6.7->langchain-community)\n",
            "  Downloading mypy_extensions-1.1.0-py3-none-any.whl.metadata (1.1 kB)\n",
            "Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.12/dist-packages (from gitdb<5,>=4.0.1->gitpython<4,>=3.1.9->mlflow-skinny<3,>=1.27.0->azure-ai-generative) (5.0.2)\n",
            "Requirement already satisfied: googleapis-common-protos<2.0.0,>=1.56.2 in /usr/local/lib/python3.12/dist-packages (from google-api-core<3.0.0,>=1.0.0->opencensus<1.0.0,>=0.11.4->opencensus-ext-azure~=1.0->azure-ai-generative) (1.70.0)\n",
            "Requirement already satisfied: proto-plus<2.0.0,>=1.22.3 in /usr/local/lib/python3.12/dist-packages (from google-api-core<3.0.0,>=1.0.0->opencensus<1.0.0,>=0.11.4->opencensus-ext-azure~=1.0->azure-ai-generative) (1.26.1)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.12/dist-packages (from google-auth~=2.0->databricks-sdk<1,>=0.20.0->mlflow-skinny<3,>=1.27.0->azure-ai-generative) (0.4.2)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.12/dist-packages (from google-auth~=2.0->databricks-sdk<1,>=0.20.0->mlflow-skinny<3,>=1.27.0->azure-ai-generative) (4.9.1)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.12/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0,>=0.12->gradio) (0.1.2)\n",
            "Collecting azure-core-tracing-opentelemetry~=1.0.0b11 (from azure-monitor-opentelemetry->azure-ai-ml>=1.14.0->azure-ai-resources>=1.0.0b7->azure-ai-generative)\n",
            "  Downloading azure_core_tracing_opentelemetry-1.0.0b12-py3-none-any.whl.metadata (11 kB)\n",
            "Collecting azure-monitor-opentelemetry-exporter~=1.0.0b41 (from azure-monitor-opentelemetry->azure-ai-ml>=1.14.0->azure-ai-resources>=1.0.0b7->azure-ai-generative)\n",
            "  Downloading azure_monitor_opentelemetry_exporter-1.0.0b42-py2.py3-none-any.whl.metadata (33 kB)\n",
            "Collecting opentelemetry-instrumentation-django~=0.57b0 (from azure-monitor-opentelemetry->azure-ai-ml>=1.14.0->azure-ai-resources>=1.0.0b7->azure-ai-generative)\n",
            "  Downloading opentelemetry_instrumentation_django-0.58b0-py3-none-any.whl.metadata (2.3 kB)\n",
            "Collecting opentelemetry-instrumentation-fastapi~=0.57b0 (from azure-monitor-opentelemetry->azure-ai-ml>=1.14.0->azure-ai-resources>=1.0.0b7->azure-ai-generative)\n",
            "  Downloading opentelemetry_instrumentation_fastapi-0.58b0-py3-none-any.whl.metadata (2.2 kB)\n",
            "Collecting opentelemetry-instrumentation-flask~=0.57b0 (from azure-monitor-opentelemetry->azure-ai-ml>=1.14.0->azure-ai-resources>=1.0.0b7->azure-ai-generative)\n",
            "  Downloading opentelemetry_instrumentation_flask-0.58b0-py3-none-any.whl.metadata (2.2 kB)\n",
            "Collecting opentelemetry-instrumentation-psycopg2~=0.57b0 (from azure-monitor-opentelemetry->azure-ai-ml>=1.14.0->azure-ai-resources>=1.0.0b7->azure-ai-generative)\n",
            "  Downloading opentelemetry_instrumentation_psycopg2-0.58b0-py3-none-any.whl.metadata (2.1 kB)\n",
            "Collecting opentelemetry-instrumentation-requests~=0.57b0 (from azure-monitor-opentelemetry->azure-ai-ml>=1.14.0->azure-ai-resources>=1.0.0b7->azure-ai-generative)\n",
            "  Downloading opentelemetry_instrumentation_requests-0.58b0-py3-none-any.whl.metadata (2.6 kB)\n",
            "Collecting opentelemetry-instrumentation-urllib~=0.57b0 (from azure-monitor-opentelemetry->azure-ai-ml>=1.14.0->azure-ai-resources>=1.0.0b7->azure-ai-generative)\n",
            "  Downloading opentelemetry_instrumentation_urllib-0.58b0-py3-none-any.whl.metadata (3.4 kB)\n",
            "Collecting opentelemetry-instrumentation-urllib3~=0.57b0 (from azure-monitor-opentelemetry->azure-ai-ml>=1.14.0->azure-ai-resources>=1.0.0b7->azure-ai-generative)\n",
            "  Downloading opentelemetry_instrumentation_urllib3-0.58b0-py3-none-any.whl.metadata (4.2 kB)\n",
            "Collecting opentelemetry-resource-detector-azure~=0.1.5 (from azure-monitor-opentelemetry->azure-ai-ml>=1.14.0->azure-ai-resources>=1.0.0b7->azure-ai-generative)\n",
            "  Downloading opentelemetry_resource_detector_azure-0.1.5-py3-none-any.whl.metadata (5.3 kB)\n",
            "Collecting fixedint==0.1.6 (from azure-monitor-opentelemetry-exporter~=1.0.0b41->azure-monitor-opentelemetry->azure-ai-ml>=1.14.0->azure-ai-resources>=1.0.0b7->azure-ai-generative)\n",
            "  Downloading fixedint-0.1.6-py3-none-any.whl.metadata (4.8 kB)\n",
            "Collecting opentelemetry-instrumentation-wsgi==0.58b0 (from opentelemetry-instrumentation-django~=0.57b0->azure-monitor-opentelemetry->azure-ai-ml>=1.14.0->azure-ai-resources>=1.0.0b7->azure-ai-generative)\n",
            "  Downloading opentelemetry_instrumentation_wsgi-0.58b0-py3-none-any.whl.metadata (2.1 kB)\n",
            "Collecting opentelemetry-instrumentation==0.58b0 (from opentelemetry-instrumentation-django~=0.57b0->azure-monitor-opentelemetry->azure-ai-ml>=1.14.0->azure-ai-resources>=1.0.0b7->azure-ai-generative)\n",
            "  Downloading opentelemetry_instrumentation-0.58b0-py3-none-any.whl.metadata (7.1 kB)\n",
            "Collecting opentelemetry-util-http==0.58b0 (from opentelemetry-instrumentation-django~=0.57b0->azure-monitor-opentelemetry->azure-ai-ml>=1.14.0->azure-ai-resources>=1.0.0b7->azure-ai-generative)\n",
            "  Downloading opentelemetry_util_http-0.58b0-py3-none-any.whl.metadata (2.6 kB)\n",
            "Requirement already satisfied: wrapt<2.0.0,>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from opentelemetry-instrumentation==0.58b0->opentelemetry-instrumentation-django~=0.57b0->azure-monitor-opentelemetry->azure-ai-ml>=1.14.0->azure-ai-resources>=1.0.0b7->azure-ai-generative) (1.17.3)\n",
            "Collecting opentelemetry-instrumentation-asgi==0.58b0 (from opentelemetry-instrumentation-fastapi~=0.57b0->azure-monitor-opentelemetry->azure-ai-ml>=1.14.0->azure-ai-resources>=1.0.0b7->azure-ai-generative)\n",
            "  Downloading opentelemetry_instrumentation_asgi-0.58b0-py3-none-any.whl.metadata (2.0 kB)\n",
            "Collecting asgiref~=3.0 (from opentelemetry-instrumentation-asgi==0.58b0->opentelemetry-instrumentation-fastapi~=0.57b0->azure-monitor-opentelemetry->azure-ai-ml>=1.14.0->azure-ai-resources>=1.0.0b7->azure-ai-generative)\n",
            "  Downloading asgiref-3.9.2-py3-none-any.whl.metadata (9.3 kB)\n",
            "Collecting opentelemetry-instrumentation-dbapi==0.58b0 (from opentelemetry-instrumentation-psycopg2~=0.57b0->azure-monitor-opentelemetry->azure-ai-ml>=1.14.0->azure-ai-resources>=1.0.0b7->azure-ai-generative)\n",
            "  Downloading opentelemetry_instrumentation_dbapi-0.58b0-py3-none-any.whl.metadata (2.0 kB)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.6.1 in /usr/local/lib/python3.12/dist-packages (from pyasn1-modules>=0.2.1->google-auth~=2.0->databricks-sdk<1,>=0.20.0->mlflow-skinny<3,>=1.27.0->azure-ai-generative) (0.6.1)\n",
            "Downloading pypdf-6.1.1-py3-none-any.whl (323 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m323.5/323.5 kB\u001b[0m \u001b[31m14.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading docx2txt-0.9-py3-none-any.whl (4.0 kB)\n",
            "Downloading azure_ai_generative-1.0.0b11-py3-none-any.whl (1.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m17.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading langchain_community-0.3.30-py3-none-any.whl (2.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m56.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading azure_ai_evaluation-1.11.1-py3-none-any.whl (1.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m42.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading azure_ai_projects-1.0.0-py3-none-any.whl (115 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m115.2/115.2 kB\u001b[0m \u001b[31m6.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading semantic_kernel-1.37.0-py3-none-any.whl (893 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m893.2/893.2 kB\u001b[0m \u001b[31m39.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading aiortc-1.13.0-py3-none-any.whl (92 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m92.9/92.9 kB\u001b[0m \u001b[31m7.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading azure_ai_agents-1.2.0b5-py3-none-any.whl (217 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m217.9/217.9 kB\u001b[0m \u001b[31m23.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading azure_ai_resources-1.0.0b9-py3-none-any.whl (2.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.1/2.1 MB\u001b[0m \u001b[31m14.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading azure_core-1.35.1-py3-none-any.whl (211 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.8/211.8 kB\u001b[0m \u001b[31m18.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading azure_identity-1.25.0-py3-none-any.whl (190 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m190.9/190.9 kB\u001b[0m \u001b[31m21.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading azure_storage_blob-12.26.0-py3-none-any.whl (412 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m412.9/412.9 kB\u001b[0m \u001b[31m40.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading cloudevents-1.12.0-py3-none-any.whl (55 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m55.8/55.8 kB\u001b[0m \u001b[31m6.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading dataclasses_json-0.6.7-py3-none-any.whl (28 kB)\n",
            "Downloading isodate-0.7.2-py3-none-any.whl (22 kB)\n",
            "Downloading mlflow_skinny-2.22.2-py3-none-any.whl (6.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.3/6.3 MB\u001b[0m \u001b[31m84.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading msrest-0.7.1-py3-none-any.whl (85 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m85.4/85.4 kB\u001b[0m \u001b[31m9.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading openapi_core-0.19.5-py3-none-any.whl (106 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m106.6/106.6 kB\u001b[0m \u001b[31m11.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading opencensus_ext_azure-1.1.15-py2.py3-none-any.whl (43 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.2/43.2 kB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading opencensus_ext_logging-0.1.1-py2.py3-none-any.whl (4.0 kB)\n",
            "Downloading packaging-24.2-py3-none-any.whl (65 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m65.5/65.5 kB\u001b[0m \u001b[31m7.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading prance-25.4.8.0-py3-none-any.whl (36 kB)\n",
            "Downloading requests-2.32.5-py3-none-any.whl (64 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m64.7/64.7 kB\u001b[0m \u001b[31m6.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading ruamel.yaml-0.18.15-py3-none-any.whl (119 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m119.7/119.7 kB\u001b[0m \u001b[31m13.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading aioice-0.10.1-py3-none-any.whl (24 kB)\n",
            "Downloading av-14.4.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (35.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m35.5/35.5 MB\u001b[0m \u001b[31m24.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading azure_ai_ml-1.29.0-py3-none-any.whl (13.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.2/13.2 MB\u001b[0m \u001b[31m75.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading azure_mgmt_resource-22.0.0-py3-none-any.whl (2.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.4/2.4 MB\u001b[0m \u001b[31m25.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading cryptography-46.0.2-cp311-abi3-manylinux_2_34_x86_64.whl (4.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.5/4.5 MB\u001b[0m \u001b[31m57.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading databricks_sdk-0.67.0-py3-none-any.whl (718 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m718.4/718.4 kB\u001b[0m \u001b[31m29.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading deprecation-2.1.0-py2.py3-none-any.whl (11 kB)\n",
            "Downloading jsonschema_path-0.3.4-py3-none-any.whl (14 kB)\n",
            "Downloading marshmallow-3.26.1-py3-none-any.whl (50 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.9/50.9 kB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading msal-1.34.0-py3-none-any.whl (116 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m117.0/117.0 kB\u001b[0m \u001b[31m10.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading msal_extensions-1.3.1-py3-none-any.whl (20 kB)\n",
            "Downloading openapi_schema_validator-0.6.3-py3-none-any.whl (8.8 kB)\n",
            "Downloading openapi_spec_validator-0.7.2-py3-none-any.whl (39 kB)\n",
            "Downloading opencensus-0.11.4-py2.py3-none-any.whl (128 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m128.2/128.2 kB\u001b[0m \u001b[31m6.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pyee-13.0.0-py3-none-any.whl (15 kB)\n",
            "Downloading pylibsrtp-0.12.0-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.2/2.2 MB\u001b[0m \u001b[31m73.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pyopenssl-25.3.0-py3-none-any.whl (57 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m57.3/57.3 kB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading ruamel.yaml.clib-0.2.14-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (753 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m753.1/753.1 kB\u001b[0m \u001b[31m64.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading typing_inspect-0.9.0-py3-none-any.whl (8.8 kB)\n",
            "Downloading werkzeug-3.1.1-py3-none-any.whl (224 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m224.4/224.4 kB\u001b[0m \u001b[31m25.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading parse-1.20.2-py2.py3-none-any.whl (20 kB)\n",
            "Downloading azure_common-1.1.28-py2.py3-none-any.whl (14 kB)\n",
            "Downloading azure_mgmt_core-1.6.0-py3-none-any.whl (29 kB)\n",
            "Downloading azure_storage_file_datalake-12.21.0-py3-none-any.whl (264 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m264.1/264.1 kB\u001b[0m \u001b[31m27.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading colorama-0.4.6-py2.py3-none-any.whl (25 kB)\n",
            "Downloading dnspython-2.8.0-py3-none-any.whl (331 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m331.1/331.1 kB\u001b[0m \u001b[31m32.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading ifaddr-0.2.0-py3-none-any.whl (12 kB)\n",
            "Downloading lazy_object_proxy-1.12.0-cp312-cp312-manylinux1_x86_64.manylinux_2_28_x86_64.manylinux_2_5_x86_64.whl (71 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m71.5/71.5 kB\u001b[0m \u001b[31m7.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading mypy_extensions-1.1.0-py3-none-any.whl (5.0 kB)\n",
            "Downloading opencensus_context-0.1.3-py2.py3-none-any.whl (5.1 kB)\n",
            "Downloading pathable-0.4.4-py3-none-any.whl (9.6 kB)\n",
            "Downloading pydash-8.0.5-py3-none-any.whl (102 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m102.1/102.1 kB\u001b[0m \u001b[31m12.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading strictyaml-1.7.3-py3-none-any.whl (123 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m123.9/123.9 kB\u001b[0m \u001b[31m14.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading azure_monitor_opentelemetry-1.8.1-py3-none-any.whl (27 kB)\n",
            "Downloading azure_storage_file_share-12.22.0-py3-none-any.whl (291 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m291.3/291.3 kB\u001b[0m \u001b[31m24.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading azure_core_tracing_opentelemetry-1.0.0b12-py3-none-any.whl (11 kB)\n",
            "Downloading azure_monitor_opentelemetry_exporter-1.0.0b42-py2.py3-none-any.whl (183 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m183.3/183.3 kB\u001b[0m \u001b[31m20.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading fixedint-0.1.6-py3-none-any.whl (12 kB)\n",
            "Downloading opentelemetry_instrumentation_django-0.58b0-py3-none-any.whl (19 kB)\n",
            "Downloading opentelemetry_instrumentation-0.58b0-py3-none-any.whl (33 kB)\n",
            "Downloading opentelemetry_instrumentation_wsgi-0.58b0-py3-none-any.whl (14 kB)\n",
            "Downloading opentelemetry_util_http-0.58b0-py3-none-any.whl (7.7 kB)\n",
            "Downloading opentelemetry_instrumentation_fastapi-0.58b0-py3-none-any.whl (13 kB)\n",
            "Downloading opentelemetry_instrumentation_asgi-0.58b0-py3-none-any.whl (16 kB)\n",
            "Downloading opentelemetry_instrumentation_flask-0.58b0-py3-none-any.whl (14 kB)\n",
            "Downloading opentelemetry_instrumentation_psycopg2-0.58b0-py3-none-any.whl (10 kB)\n",
            "Downloading opentelemetry_instrumentation_dbapi-0.58b0-py3-none-any.whl (12 kB)\n",
            "Downloading opentelemetry_instrumentation_requests-0.58b0-py3-none-any.whl (12 kB)\n",
            "Downloading opentelemetry_instrumentation_urllib-0.58b0-py3-none-any.whl (12 kB)\n",
            "Downloading opentelemetry_instrumentation_urllib3-0.58b0-py3-none-any.whl (13 kB)\n",
            "Downloading opentelemetry_resource_detector_azure-0.1.5-py3-none-any.whl (14 kB)\n",
            "Downloading asgiref-3.9.2-py3-none-any.whl (23 kB)\n",
            "Building wheels for collected packages: pybars4, PyMeta3\n",
            "  Building wheel for pybars4 (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pybars4: filename=pybars4-0.9.13-py3-none-any.whl size=14340 sha256=4a54748b0f65e4ea44c7a9937e8512e473c5800aa0a293ac6f652c2f28721be4\n",
            "  Stored in directory: /root/.cache/pip/wheels/75/2d/da/c75b2fc7b00dc9c154dff65f689a318e7d24b44c612c2b21f1\n",
            "  Building wheel for PyMeta3 (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for PyMeta3: filename=PyMeta3-0.5.1-py3-none-any.whl size=16449 sha256=8d4f0cb28509ae420a7d3dadc23d15cdfcc74125a2233847af82eea14d781d7b\n",
            "  Stored in directory: /root/.cache/pip/wheels/5a/9b/2c/81b7551d2c05a482817e6c3b3d2f8ad2a0229db874c4bc6346\n",
            "Successfully built pybars4 PyMeta3\n",
            "Installing collected packages: PyMeta3, parse, opencensus-context, ifaddr, fixedint, docx2txt, azure-common, werkzeug, ruamel.yaml.clib, requests, pypdf, pyee, pydash, pybars4, pathable, packaging, opentelemetry-util-http, mypy-extensions, lazy-object-proxy, isodate, dnspython, colorama, av, asgiref, typing-inspect, strictyaml, ruamel.yaml, pylibsrtp, marshmallow, jsonschema-path, deprecation, cryptography, azure-core, aioice, pyopenssl, prance, msrest, dataclasses-json, databricks-sdk, cloudevents, azure-storage-file-share, azure-storage-blob, azure-mgmt-core, azure-core-tracing-opentelemetry, azure-ai-agents, opentelemetry-instrumentation, opencensus, openapi-schema-validator, msal, azure-storage-file-datalake, azure-mgmt-resource, azure-ai-projects, aiortc, opentelemetry-resource-detector-azure, opentelemetry-instrumentation-wsgi, opentelemetry-instrumentation-urllib3, opentelemetry-instrumentation-urllib, opentelemetry-instrumentation-requests, opentelemetry-instrumentation-dbapi, opentelemetry-instrumentation-asgi, opencensus-ext-logging, openapi-spec-validator, msal-extensions, mlflow-skinny, opentelemetry-instrumentation-psycopg2, opentelemetry-instrumentation-flask, opentelemetry-instrumentation-fastapi, opentelemetry-instrumentation-django, openapi_core, azure-identity, semantic-kernel, opencensus-ext-azure, langchain-community, azure-monitor-opentelemetry-exporter, azure-ai-evaluation, azure-monitor-opentelemetry, azure-ai-ml, azure-ai-resources, azure-ai-generative\n",
            "  Attempting uninstall: werkzeug\n",
            "    Found existing installation: Werkzeug 3.1.3\n",
            "    Uninstalling Werkzeug-3.1.3:\n",
            "      Successfully uninstalled Werkzeug-3.1.3\n",
            "  Attempting uninstall: requests\n",
            "    Found existing installation: requests 2.32.4\n",
            "    Uninstalling requests-2.32.4:\n",
            "      Successfully uninstalled requests-2.32.4\n",
            "  Attempting uninstall: packaging\n",
            "    Found existing installation: packaging 25.0\n",
            "    Uninstalling packaging-25.0:\n",
            "      Successfully uninstalled packaging-25.0\n",
            "  Attempting uninstall: cryptography\n",
            "    Found existing installation: cryptography 43.0.3\n",
            "    Uninstalling cryptography-43.0.3:\n",
            "      Successfully uninstalled cryptography-43.0.3\n",
            "  Attempting uninstall: pyopenssl\n",
            "    Found existing installation: pyOpenSSL 24.2.1\n",
            "    Uninstalling pyOpenSSL-24.2.1:\n",
            "      Successfully uninstalled pyOpenSSL-24.2.1\n"
          ]
        }
      ],
      "source": [
        "# Install Dependencies\n",
        "! pip install langchain pypdf docx2txt pandas openai gradio azure-ai-generative langchain-community azure-ai-evaluation azure-ai-projects semantic-kernel"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wFe_KJjivxlO"
      },
      "source": [
        "# Step 2 : Imports and Configuration\n",
        "\n",
        "This code cell sets up the necessary imports and configurations for processing contract documents, interacting with OpenAI models, and using Azure AI Evaluation tools.\n",
        "\n",
        "```\n",
        "| Import / Library                  | Purpose                                                                                       |\n",
        "|---------------------------------|------------------------------------------------------------------------------------------------|\n",
        "| `os`, `io`, `tempfile`, `re`, `json`, `time` | Standard Python libraries for file handling, string processing, JSON manipulation, and timing.\n",
        "| `PyPDFLoader`, `Docx2txtLoader`, `TextLoader` | LangChain loaders to read and extract text from PDF, DOCX, and text files.                    \n",
        "| `RecursiveCharacterTextSplitter` | Splits large documents into smaller chunks for better processing by AI models.                 \n",
        "| `Document`                      | LangChain object representing textual documents with associated metadata.                      \n",
        "| `pandas`                       | For organizing data and evaluation results in tabular format.                                 \n",
        "| `gradio`                       | To build interactive user interfaces and demos.                                              \n",
        "| `OpenAI` and error classes     | OpenAI Python SDK for making API calls and handling errors like rate limits or connection issues.\n",
        "| `GroundednessEvaluator`, `CoherenceEvaluator`, `RelevanceEvaluator`, `FluencyEvaluator` | Azure AI Evaluation SDK classes to measure AI-generated content’s quality on various dimensions.\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "This setup prepares the environment to load contract documents, process them, generate AI responses, and evaluate those responses using Azure AI Evaluation’s built-in metrics.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sVm26qb5jJma"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import io\n",
        "import tempfile\n",
        "import re\n",
        "import json\n",
        "from langchain.document_loaders import PyPDFLoader, Docx2txtLoader, TextLoader\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "import pandas as pd\n",
        "from langchain.schema import Document\n",
        "from openai import OpenAI\n",
        "from openai import APIError, APIConnectionError, RateLimitError\n",
        "import time\n",
        "import gradio as gr\n",
        "\n",
        "# Azure AI Evaluation imports\n",
        "from azure.ai.evaluation import GroundednessEvaluator, CoherenceEvaluator, RelevanceEvaluator, FluencyEvaluator"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iALEswltyjmZ"
      },
      "source": [
        "## Step 3 : OpenAI and Azure AI Configuration\n",
        "\n",
        "### OpenAI Client Initialization\n",
        "\n",
        "> 1. **OpenAI API Key**  \n",
        ">    To use the OpenAI client, you need an API key.  \n",
        ">    👉 Follow this guide to get your API key:  \n",
        ">    [How to get your own OpenAI API key (Medium)](https://medium.com/@lorenzozar/how-to-get-your-own-openai-api-key-f4d44e60c327)\n",
        "\n",
        "---\n",
        "\n",
        "### Azure AI Project Configuration\n",
        "\n",
        "The `azure_ai_project` dictionary holds metadata for identifying and accessing a specific Azure AI project. This includes:\n",
        "\n",
        "- `subscription_id`: The unique identifier of your Azure subscription.\n",
        "- `resource_group_name`: The name of the resource group that contains your Azure resources.\n",
        "- `project_name`: The name of the Azure AI project or workspace.\n",
        "\n",
        "<br>\n",
        "\n",
        "> **To configure the Azure AI project:**  \n",
        "> 👉 Use this official Microsoft guide to create an Azure AI project:  \n",
        "> [Create Azure AI Projects (Microsoft Docs)](https://learn.microsoft.com/en-us/azure/ai-foundry/how-to/create-projects?tabs=ai-foundry&pivots=fdp-project)\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "### Azure OpenAI Model Configuration\n",
        "\n",
        "The `model_config` dictionary contains connection details required to interact with an Azure-hosted OpenAI deployment.\n",
        "\n",
        "> - **Follow the instructions here:**  \n",
        "> [Create a Foundry Project](../Create_Azure_Project_Instruction/Readme.md)\n",
        "\n",
        "\n",
        "- `azure_endpoint`: The base URL of the Azure OpenAI resource.\n",
        "- `api_key`: The API key for authentication against the Azure OpenAI endpoint.\n",
        "- `azure_deployment`: The specific deployment name of the OpenAI model within Azure.\n",
        "- `api_version`: The version of the Azure OpenAI API to be used (e.g., `2024-02-15-preview`).\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "> ❗ **Important:**  \n",
        "> You **must** have both your **OpenAI API key** and your **Azure AI project + model configuration** completed.  \n",
        "> Without these keys and setup, the application **will not run** and you **cannot proceed** further.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ALjteZupjOBD"
      },
      "outputs": [],
      "source": [
        "# Initialize OpenAI client (replace with your own API key securely)\n",
        "# Follow this link to get your api key : https://medium.com/@lorenzozar/how-to-get-your-own-openai-api-key-f4d44e60c327)\n",
        "client = OpenAI(api_key='YOUR_OPENAI_API_KEY')\n",
        "\n",
        "\n",
        "# Initialize Azure AI project configuration (replace placeholders with your own values)\n",
        "# Follow this link create a azure ai project  : https://learn.microsoft.com/en-us/azure/ai-foundry/how-to/create-projects?tabs=ai-foundry&pivots=fdp-project\n",
        "azure_ai_project = {\n",
        "    \"subscription_id\": \"your-subscription-id\",\n",
        "    \"resource_group_name\": \"your-resource-group-name\",\n",
        "    \"project_name\": \"your-project-name\",\n",
        "}\n",
        "\n",
        "\n",
        "# Azure OpenAI model configuration (replace placeholders with your own values)\n",
        "# > **Note:** Please go to the [Prerequisites - step 3] section to deploy a model in ai foundary and obtain the required keys.\n",
        "\n",
        "model_config = {\n",
        "    \"azure_endpoint\": \"your-azure-endpoint\",\n",
        "    \"api_key\": \"YOUR_AZURE_API_KEY\",\n",
        "    \"azure_deployment\": \"your-deployment-name\",\n",
        "    \"api_version\": \"your-api-version\"\n",
        "}\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UR0ZtpwBy3IQ"
      },
      "source": [
        "### Step 4 : Azure Evaluator Initialization\n",
        "\n",
        "Initializes multiple evaluation modules from Azure AI to assess different quality aspects of text generation (e.g., model outputs or document parsing).\n",
        "\n",
        "- **`GroundednessEvaluator`**: Measures how well the generated content is grounded in the source material.\n",
        "- **`CoherenceEvaluator`**: Evaluates logical flow and consistency between sentences.\n",
        "- **`RelevanceEvaluator`**: Checks how relevant the output is to the input or expected context.\n",
        "- **`FluencyEvaluator`**: Assesses the grammatical and linguistic quality of the content.\n",
        "\n",
        "Each evaluator is configured using the `model_config` dictionary, which must contain Azure OpenAI endpoint, API key, deployment name, and API version.\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "### Key Terms to Extract\n",
        "\n",
        "Defines a list of key legal or contractual terms to identify and extract from the contract.\n",
        "\n",
        "- **`\"Product Name\"`**: The name of the product being described or licensed.\n",
        "- **`\"Limitation of Liability In Months\"`**: The time limit for legal liability in the agreement.\n",
        "- **`\"Governing Law\"`**: Specifies which jurisdiction’s laws apply to the contract."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2mCv9tgAjQkm"
      },
      "outputs": [],
      "source": [
        "# Initialize Azure evaluators\n",
        "groundedness_eval = GroundednessEvaluator(model_config=model_config)\n",
        "coherence_eval = CoherenceEvaluator(model_config=model_config)\n",
        "relevance_eval = RelevanceEvaluator(model_config=model_config)\n",
        "fluency_eval = FluencyEvaluator(model_config=model_config)\n",
        "\n",
        "# Key terms to extract\n",
        "KEY_TERMS = [\n",
        "    \"Product Name\",\n",
        "    \"Limitation of Liability In Months\",\n",
        "    \"Governing Law\"\n",
        "]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5UcGbxHzzQ6a"
      },
      "source": [
        "## Step 5  : File Text Extraction Helper\n",
        "\n",
        "### Function: `extract_text_from_file(file_path)`\n",
        "\n",
        "This helper function handles the extraction of text from various file types, converting them into a consistent structure for downstream processing (e.g., with LangChain or LLM pipelines).\n",
        "\n",
        "#### Parameters\n",
        "\n",
        "- **`file_path`** *(str)*: Path to the input file whose content needs to be extracted.\n",
        "\n",
        "#### Returns\n",
        "\n",
        "- **`text`** *(str)*: Full extracted text from the file, as a single string.\n",
        "- **`docs`** *(list)*: A list of document-like objects, each having a `page_content` attribute for structured processing.\n",
        "\n",
        "---\n",
        "\n",
        "### Supported File Types\n",
        "\n",
        "| File Type | Loader Used         | Description                              |\n",
        "|-----------|---------------------|------------------------------------------|\n",
        "| `.pdf`    | `PyPDFLoader`       | Extracts page-wise text from PDF         |\n",
        "| `.docx`, `.doc` | `Docx2txtLoader` | Reads content from Word documents        |\n",
        "| `.txt`    | `TextLoader`        | Loads plain text from text files         |\n",
        "| `.csv`    | `pandas.read_csv`   | Converts CSV rows into a text string     |\n",
        "\n",
        "\n",
        "\n",
        "> ⚠️ **Warning:** If the file type is not recognized, the function raises a `ValueError`.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DAyoPNnajS0s"
      },
      "outputs": [],
      "source": [
        "def extract_text_from_file(file_path):\n",
        "    # Get the file extension and convert it to lowercase\n",
        "    ext = os.path.splitext(file_path)[1].lower()\n",
        "\n",
        "    # Handle PDF files\n",
        "    if ext == \".pdf\":\n",
        "        loader = PyPDFLoader(file_path)  # Use PyPDFLoader to read PDF\n",
        "        docs = loader.load()  # Load document into LangChain Document objects\n",
        "        text = \"\\n\".join([doc.page_content for doc in docs])  # Combine all page content\n",
        "\n",
        "    # Handle Word documents (.docx, .doc)\n",
        "    elif ext in [\".docx\", \".doc\"]:\n",
        "        loader = Docx2txtLoader(file_path)  # Use Docx2txtLoader for Word files\n",
        "        docs = loader.load()\n",
        "        text = \"\\n\".join([doc.page_content for doc in docs])\n",
        "\n",
        "    # Handle plain text files\n",
        "    elif ext in [\".txt\"]:\n",
        "        loader = TextLoader(file_path)  # Use TextLoader for .txt files\n",
        "        docs = loader.load()\n",
        "        text = \"\\n\".join([doc.page_content for doc in docs])\n",
        "\n",
        "    # Handle CSV files\n",
        "    elif ext == \".csv\":\n",
        "        df = pd.read_csv(file_path)  # Read CSV using pandas\n",
        "        text = df.to_string(index=False)  # Convert DataFrame to plain string\n",
        "        # Wrap in a dummy doc-like object to keep consistent structure\n",
        "        docs = [type('Doc', (object,), {'page_content': text})()]\n",
        "\n",
        "    # Unsupported file types\n",
        "    else:\n",
        "        raise ValueError(\"Unsupported file type\")\n",
        "\n",
        "    # Return both raw text and structured docs for further processing\n",
        "    return text, docs  # docs may include page-level details"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0KEWSlk40gDe"
      },
      "source": [
        "## Step 6 : Key Term Extraction with LLM and JSON Parsing\n",
        "\n",
        "### Function: `extract_key_terms(text, key_terms)`\n",
        "\n",
        "This function extracts specific legal or contractual terms from a text document using a large language model (LLM). It prompts the LLM to return structured JSON responses and parses them with resilience against formatting issues.\n",
        "\n",
        "---\n",
        "\n",
        "### 📥 Parameters\n",
        "\n",
        "- **`text`** *(str)*: Full contract or document text to be analyzed.\n",
        "- **`key_terms`** *(list of str)*: List of term names (e.g., `\"Governing Law\"`) to extract from the document.\n",
        "\n",
        "---\n",
        "\n",
        "### 📤 Returns\n",
        "\n",
        "- **`results`** *(dict)*: Dictionary mapping each key term to a dictionary containing:\n",
        "  - `\"Value\"`: The extracted value or `\"Not found\"`.\n",
        "  - `\"page_number\"`: The page number where the term was found (if available).\n",
        "\n",
        "Example:\n",
        "```json\n",
        "{\n",
        "  \"Governing Law\": {\n",
        "    \"Value\": \"California\",\n",
        "    \"page_number\": \"5\"\n",
        "  }\n",
        "}\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hUWx97BPjXAt"
      },
      "outputs": [],
      "source": [
        "def extract_key_terms(text, key_terms):\n",
        "\n",
        "    def safe_json_parse(response_text):\n",
        "        \"\"\"Safely parse JSON from LLM response with fallback strategies.\"\"\"\n",
        "        # Strategy 1: Try direct JSON parsing\n",
        "        try:\n",
        "            return json.loads(response_text.strip())\n",
        "        except json.JSONDecodeError:\n",
        "            pass\n",
        "\n",
        "        # Strategy 2: Extract from code blocks\n",
        "        json_patterns = [\n",
        "            r'```json\\s*(.*?)\\s*```',\n",
        "            r'```\\s*(.*?)\\s*```',\n",
        "            r'\\{.*\\}'\n",
        "        ]\n",
        "\n",
        "        for pattern in json_patterns:\n",
        "            json_match = re.search(pattern, response_text, re.DOTALL)\n",
        "            if json_match:\n",
        "                try:\n",
        "                    return json.loads(json_match.group(1).strip() if 'json' in pattern else json_match.group(0))\n",
        "                except json.JSONDecodeError:\n",
        "                    continue\n",
        "\n",
        "        # Fallback: return default structure\n",
        "        return {\"Value\": \"Not found\", \"Page Number\": None, \"Section\": None}\n",
        "\n",
        "    results = {}\n",
        "    for term in key_terms:\n",
        "        prompt = (\n",
        "            f\"Act as a legal expert. From this contract text, extract the value for '{term}'.\\n\\n\"\n",
        "            f\"Contract Text: {text}\\n\\n\"\n",
        "            f\"Instructions:\\n\"\n",
        "            f\"1. Provide a one-word answer for '{term}' if found\\n\"\n",
        "            f\"2. Include page number if available\\n\"\n",
        "            f\"3. If not found, use 'Not found'\\n\\n\"\n",
        "            f\"Return ONLY valid JSON in this exact format:\\n\"\n",
        "            f'{{\"Value\": \"your_answer\", \"Page Number\": \"page_number_or_null\", \"Section\": \"section_name\"}}'\n",
        "        )\n",
        "\n",
        "        try:\n",
        "            completion = client.chat.completions.create(\n",
        "                model=\"gpt-4o-mini\",  # Fixed model name\n",
        "                messages=[\n",
        "                    {\"role\": \"system\", \"content\": \"You are a legal contract analysis assistant. Always return valid JSON only.\"},\n",
        "                    {\"role\": \"user\", \"content\": prompt}\n",
        "                ],\n",
        "                temperature=0.1  # Lower temperature for consistency\n",
        "            )\n",
        "            answer = completion.choices[0].message.content\n",
        "            print(\"****************LLM Answer*******************\")\n",
        "            print(answer)\n",
        "            print(\"*********************************************\")\n",
        "\n",
        "            # Use safe JSON parsing\n",
        "            parsed_response = safe_json_parse(answer)\n",
        "            print(f\"DEBUG: Parsed JSON successfully: {parsed_response}\")\n",
        "\n",
        "            value = parsed_response.get(\"Value\", \"Not found\")\n",
        "            print(f\"DEBUG: Extracted value: {value}\")\n",
        "\n",
        "            # Extract page number with multiple fallback options\n",
        "            page_number = (\n",
        "                parsed_response.get(\"Page Number\") or\n",
        "                parsed_response.get(\"PageNumber\") or\n",
        "                parsed_response.get(\"page_number\")\n",
        "            )\n",
        "            print(f\"DEBUG: Direct page number extraction: {page_number}\")\n",
        "\n",
        "            # If not found, try extracting from Section field\n",
        "            if not page_number:\n",
        "                section = parsed_response.get(\"Section\", \"\")\n",
        "                print(f\"DEBUG: Section field content: {section}\")\n",
        "                if section and \"Page\" in str(section):\n",
        "                    page_match = re.search(r'Page (\\d+)', str(section))\n",
        "                    if page_match:\n",
        "                        page_number = page_match.group(1)\n",
        "                        print(f\"DEBUG: Extracted page number from section: {page_number}\")\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"DEBUG: API call or parsing failed: {e}\")\n",
        "            value = \"Error\"\n",
        "            page_number = None\n",
        "\n",
        "        final_result = {\"Value\": value, \"page_number\": page_number}\n",
        "        print(f\"DEBUG: Final result for term '{term}': {final_result}\")\n",
        "        print(\"=\" * 60)\n",
        "\n",
        "        results[term] = final_result\n",
        "\n",
        "    print(f\"DEBUG: All results: {results}\")\n",
        "    return results\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bjJj6G3h03BQ"
      },
      "source": [
        "## Step 7 : LLM Response Evaluation with Azure AI\n",
        "\n",
        "### Function: `azure_judge(entry, results_df)`\n",
        "\n",
        "This function evaluates a single LLM-generated response across four evaluation — **groundedness**, **coherence**, **relevance**, and **fluency** — using Azure-based evaluators. It returns an updated results DataFrame with the new evaluation appended.\n",
        "\n",
        "---\n",
        "\n",
        "### 📥 Parameters\n",
        "\n",
        "- **`entry`** *(dict)*: A dictionary containing the following fields:\n",
        "  - `\"Key Term Name\"`: Name of the key term being evaluated.\n",
        "  - `\"context\"`: Background or reference text used for grounding.\n",
        "  - `\"query\"`: The question or prompt issued to the LLM.\n",
        "  - `\"llm_response\"`: The LLM-generated output to be evaluated.\n",
        "\n",
        "- **`results_df`** *(pd.DataFrame)*: The existing DataFrame to which the evaluation result will be appended.\n",
        "\n",
        "---\n",
        "\n",
        "### 📤 Returns\n",
        "\n",
        "- **`pd.DataFrame`**: An updated DataFrame with a new row containing:\n",
        "  - Evaluation scores from all four evaluators\n",
        "  - The query, context, response, and key term name\n",
        "\n",
        "---\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PsFL3E6djZaF"
      },
      "outputs": [],
      "source": [
        "def azure_judge(entry, results_df):\n",
        "    \"\"\"Evaluate a single entry using all evaluators and add to results dataframe\"\"\"\n",
        "\n",
        "    # Format inputs for each evaluator - using llm_response instead of ground_truth\n",
        "    groundedness_input = {\n",
        "        \"query\": entry[\"query\"],\n",
        "        \"context\": entry[\"context\"],\n",
        "        \"response\": entry[\"llm_response\"]  # Changed from ground_truth to llm_response\n",
        "    }\n",
        "\n",
        "    coherence_input = {\n",
        "        \"query\": entry[\"query\"],\n",
        "        \"response\": entry[\"llm_response\"]  # Changed from ground_truth to llm_response\n",
        "    }\n",
        "\n",
        "    relevance_input = {\n",
        "        \"query\": entry[\"query\"],\n",
        "        \"response\": entry[\"llm_response\"]  # Changed from ground_truth to llm_response\n",
        "    }\n",
        "\n",
        "    fluency_input = {\n",
        "        \"response\": entry[\"llm_response\"]  # Changed from ground_truth to llm_response\n",
        "    }\n",
        "\n",
        "    try:\n",
        "        # Get all scores\n",
        "        groundedness_score = groundedness_eval(**groundedness_input)\n",
        "        coherence_score = coherence_eval(**coherence_input)\n",
        "        relevance_score = relevance_eval(**relevance_input)\n",
        "        fluency_score = fluency_eval(**fluency_input)\n",
        "\n",
        "        # Create new row for the dataframe\n",
        "        new_row = {\n",
        "            'Key Term Name': entry['Key Term Name'], # Ensure Key Term Name is passed through\n",
        "            'Context': entry['context'],\n",
        "            'Query': entry['query'],\n",
        "            'LLM Response': entry['llm_response'],\n",
        "            'Groundedness Score': groundedness_score['groundedness'],\n",
        "            'Coherence Score': coherence_score['coherence'],\n",
        "            'Relevance Score': relevance_score['relevance'],\n",
        "            'Fluency Score': fluency_score['fluency']\n",
        "        }\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error evaluating entry: {e}\")\n",
        "        # Create row with error indicators\n",
        "        new_row = {\n",
        "            'Key Term Name': entry['Key Term Name'],\n",
        "            'Context': entry['context'],\n",
        "            'Query': entry['query'],\n",
        "            'LLM Response': entry['llm_response'],\n",
        "            'Groundedness Score': None,\n",
        "            'Coherence Score': None,\n",
        "            'Relevance Score': None,\n",
        "            'Fluency Score': None\n",
        "        }\n",
        "\n",
        "    # Use concat with a pre-defined DataFrame\n",
        "    new_row_df = pd.DataFrame([new_row])\n",
        "    return pd.concat([results_df, new_row_df], ignore_index=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dTsd55ne1Syk"
      },
      "source": [
        "## Step 8 : Main Document Processing Pipeline\n",
        "\n",
        "### Function: `process_document(file)`\n",
        "\n",
        "This core function orchestrates the end-to-end workflow for document ingestion, key term extraction, and quality evaluation of AI-generated outputs using Azure Large Language Model (LLM) evaluators. It is designed to process contractual or related documents, extracting relevant terms and assessing the reliability and quality of the extracted data.\n",
        "\n",
        "---\n",
        "\n",
        "### Parameters\n",
        "\n",
        "- **`file`** *(UploadedFile or equivalent)*:  \n",
        "  The input file to be processed. Supported formats include `.pdf`, `.docx`, `.txt`, and `.csv`.\n",
        "\n",
        "---\n",
        "\n",
        "### Returns\n",
        "\n",
        "A tuple consisting of:\n",
        "\n",
        "1. **Status Message** *(str)* — Indicates success or explains errors encountered during processing.  \n",
        "2. **`extracted_df`** *(pandas.DataFrame)* — A structured table containing extracted key contractual terms, their detected values, and associated page numbers or locations within the document.  \n",
        "3. **`evaluation_summary`** *(pandas.DataFrame)* — A summary table containing quality scores generated by Azure LLM evaluators for each extracted term, measuring attributes such as groundedness, coherence, relevance, and fluency.\n",
        "\n",
        "---\n",
        "\n",
        "### Workflow Overview\n",
        "\n",
        "#### 1. File Validation  \n",
        "The function initially verifies that a valid file has been provided. If no file is detected, it terminates early with a user-friendly prompt requesting file upload.\n",
        "\n",
        "#### 2. Document Text Extraction  \n",
        "The input document is processed using the `extract_text_from_file()` utility, which supports multiple file formats. This step yields:  \n",
        "- A single concatenated string representing the full document text.  \n",
        "- A structured list of LangChain-compatible document objects (`docs`) for downstream NLP processing.\n",
        "\n",
        "> **Supported formats:** PDF, Microsoft Word (.docx), plain text (.txt), and CSV files.\n",
        "\n",
        "#### 3. Key Term Extraction  \n",
        "Utilizing prompt-based Large Language Model (LLM) techniques, the function `extract_key_terms()` is called to accurately identify and extract relevant contractual terms and their corresponding values from the processed text. The output is a dictionary mapping key terms to their detected values along with metadata such as page numbers where the terms were found.\n",
        "\n",
        "> **Example key contractual terms:** `\"Governing Law\"`, `\"Product Name\"`, `\"Termination Clause\"`.\n",
        "\n",
        "#### 4. Azure AI Evaluation  \n",
        "Each extracted key term is then evaluated for quality and reliability:  \n",
        "- An evaluation context is prepared combining the original query, relevant document context, and the LLM’s extracted value.  \n",
        "- The `azure_judge()` function invokes Azure AI evaluators to assess each response across multiple dimensions:  \n",
        "  - **Groundedness:** Degree to which the output is supported by source content.  \n",
        "  - **Coherence:** Logical consistency and clarity of the extracted response.  \n",
        "  - **Relevance:** Pertinence of the response to the specific query or clause.  \n",
        "  - **Fluency:** Language quality, readability, and grammatical correctness."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zLBu8zrljj9Y"
      },
      "outputs": [],
      "source": [
        "# Main Processing Function\n",
        "\n",
        "def process_document(file):\n",
        "    \"\"\"Main function to process document and return results\"\"\"\n",
        "\n",
        "    if file is None:\n",
        "        return \"Please upload a document first.\", None, None\n",
        "\n",
        "    try:\n",
        "        # Step 2: Extract text from document\n",
        "        print(\"Step 2: Extracting text from document...\")\n",
        "        document_text, docs = extract_text_from_file(file.name)\n",
        "\n",
        "        # Step 3: Extract key terms using LLM\n",
        "        print(\"Step 3: Extracting key terms...\")\n",
        "        key_term_results = extract_key_terms(document_text, KEY_TERMS)\n",
        "\n",
        "        # Step 4: Prepare data for Azure evaluation\n",
        "        print(\"Step 4: Preparing for Azure evaluation...\")\n",
        "        results_df = pd.DataFrame()\n",
        "\n",
        "        extracted_info = []\n",
        "\n",
        "        for term, result in key_term_results.items():\n",
        "            # Prepare entry for azure evaluation\n",
        "            entry = {\n",
        "                'Key Term Name': term,\n",
        "                'context': document_text[:2000],  # Limit context for API\n",
        "                'query': f\"Extract the {term} from this contract\",\n",
        "                'llm_response': result['Value']\n",
        "            }\n",
        "\n",
        "            # Add to extracted info for display\n",
        "            extracted_info.append({\n",
        "                'Key Term': term,\n",
        "                'Extracted Value': result['Value'],\n",
        "                'Page Number': result['page_number'] if result['page_number'] else 'N/A'\n",
        "            })\n",
        "\n",
        "            # Evaluate with Azure\n",
        "            print(f\"Evaluating {term}...\")\n",
        "            results_df = azure_judge(entry, results_df)\n",
        "\n",
        "        # Create extracted info dataframe for display\n",
        "        extracted_df = pd.DataFrame(extracted_info)\n",
        "\n",
        "        # Create evaluation summary\n",
        "        if not results_df.empty:\n",
        "            evaluation_summary = results_df[['Key Term Name', 'LLM Response', 'Groundedness Score',\n",
        "                                           'Coherence Score', 'Relevance Score', 'Fluency Score']].copy()\n",
        "        else:\n",
        "            evaluation_summary = pd.DataFrame()\n",
        "\n",
        "        return (\n",
        "            f\"✅ Document processed successfully!\\n\\nExtracted {len(key_term_results)} key terms.\",\n",
        "            extracted_df,\n",
        "            evaluation_summary\n",
        "        )\n",
        "\n",
        "    except Exception as e:\n",
        "        return f\"❌ Error processing document: {str(e)}\", None, None"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sqoxZXFY2CYy"
      },
      "source": [
        "## Step 9 : Dashboard Visualization of LLM Evaluation Scores\n",
        "\n",
        "### Function: `create_dashboard(evaluation_df)`\n",
        "\n",
        "This function generates a comprehensive three-panel dashboard using Matplotlib and Seaborn to visualize evaluation metrics of Large Language Model (LLM) outputs. It presents key quality attributes such as groundedness, coherence, relevance, and fluency across extracted terms from document analysis.\n",
        "\n",
        "---\n",
        "\n",
        "### Parameters\n",
        "\n",
        "- **`evaluation_df`** *(pandas.DataFrame)*:  \n",
        "  A DataFrame containing LLM evaluation scores, typically produced by the `process_document()` pipeline. It should include columns representing different quality metrics aligned with specific key terms.\n",
        "\n",
        "---\n",
        "\n",
        "### Returns\n",
        "\n",
        "- **`fig`** *(matplotlib.figure.Figure)*:  \n",
        "  A Matplotlib Figure object that encapsulates the dashboard's visualizations. This figure can be rendered directly in various Python UI frameworks like Streamlit or saved as an image file for reporting purposes.\n",
        "\n",
        "---\n",
        "\n",
        "### Visualization Panels\n",
        "\n",
        "#### 1. Average Evaluation Scores (Bar Chart)  \n",
        "Displays the overall average scores for the four core evaluation metrics (groundedness, coherence, relevance, fluency) aggregated across all key terms and documents.  \n",
        "> **Note:** Scores are expected to range from 0 (lowest) to 5 (highest).\n",
        "\n",
        "#### 2. Performance by Key Term (Bar Chart)  \n",
        "Aggregates and depicts scores per extracted key term, allowing users to identify specific contract clauses or topics where the LLM's performance is stronger or weaker.\n",
        "\n",
        "#### 3. Overall Performance Gauge  \n",
        "A semi-circular gauge chart illustrating the mean evaluation score across all terms and metrics.  \n",
        "- Uses categorical color coding to visually differentiate performance levels:  \n",
        "  - 🟢 Excellent (4.0 – 5.0)  \n",
        "  - 🟠 Good (3.0 – 4.0)  \n",
        "  - 🔴 Fair (2.0 – 3.0)  \n",
        "  - ⚪ Poor (< 2.0)\n",
        "\n",
        "---\n",
        "\n",
        "### Internal Logic and Features\n",
        "\n",
        "- **Data Preparation:**  \n",
        "  - Converts evaluation score columns to numeric format.  \n",
        "  - Filters out records with all missing (`NaN`) values to maintain plot accuracy.\n",
        "\n",
        "- **Styling and Palettes:**  \n",
        "  - Employs Seaborn's `\"husl\"` color palette to ensure vibrant, consistent aesthetics.  \n",
        "  - Custom colors are assigned for the gauge’s performance segments.\n",
        "\n",
        "- **Annotations and Responsiveness:**  \n",
        "  - Bar charts include value labels directly on bars for readability.  \n",
        "  - Axes and tick labels are styled for clear visualization even with multiple terms.\n",
        "\n",
        "- **Gauge Plot Implementation:**  \n",
        "  - Uses polar coordinates to simulate a semi-circular gauge.  \n",
        "  - Divides the gauge into colored bands representing performance categories.  \n",
        "  - A needle indicates the computed mean score dynamically.\n",
        "\n",
        "- **Fallback Behavior:**  \n",
        "  - In the absence of valid data, the function returns a blank figure with a user-friendly message indicating no data is available to plot.\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QQ4Ab4g6jnxv"
      },
      "outputs": [],
      "source": [
        "# Dashboard Visualization Function\n",
        "\n",
        "def create_dashboard(evaluation_df):\n",
        "    \"\"\"Create dashboard visualization of evaluation scores\"\"\"\n",
        "\n",
        "    if evaluation_df is None or evaluation_df.empty:\n",
        "        return None\n",
        "\n",
        "    import matplotlib.pyplot as plt\n",
        "    import seaborn as sns\n",
        "\n",
        "    # Set style\n",
        "    plt.style.use('default')\n",
        "    sns.set_palette(\"husl\")\n",
        "\n",
        "    # Create figure with subplots\n",
        "    fig, axes = plt.subplots(1, 3, figsize=(18, 6))\n",
        "    fig.suptitle('Document Evaluation Dashboard', fontsize=16, fontweight='bold')\n",
        "\n",
        "    # Score columns\n",
        "    score_columns = ['Groundedness Score', 'Coherence Score', 'Relevance Score', 'Fluency Score']\n",
        "\n",
        "    # Filter out None values and convert to numeric\n",
        "    plot_data = evaluation_df.copy()\n",
        "    for col in score_columns:\n",
        "        plot_data[col] = pd.to_numeric(plot_data[col], errors='coerce')\n",
        "\n",
        "    # Remove rows with all NaN scores\n",
        "    plot_data = plot_data.dropna(subset=score_columns, how='all')\n",
        "\n",
        "    if plot_data.empty:\n",
        "        # If no valid data, show message\n",
        "        fig.text(0.5, 0.5, 'No valid evaluation scores available',\n",
        "                ha='center', va='center', fontsize=20)\n",
        "        return fig\n",
        "\n",
        "    # 1. Individual Scores Bar Chart\n",
        "    ax1 = axes[0]\n",
        "    scores_avg = plot_data[score_columns].mean()\n",
        "    bars1 = ax1.bar(range(len(scores_avg)), scores_avg.values,\n",
        "                    color=['#FF6B6B', '#4ECDC4', '#45B7D1', '#96CEB4'])\n",
        "    ax1.set_xlabel('Evaluation Metrics')\n",
        "    ax1.set_ylabel('Average Score')\n",
        "    ax1.set_title('Average Evaluation Scores')\n",
        "    ax1.set_xticks(range(len(scores_avg)))\n",
        "    ax1.set_xticklabels([col.replace(' Score', '') for col in scores_avg.index], rotation=45)\n",
        "    ax1.set_ylim(0, 5)  # Assuming scores are 0-5\n",
        "\n",
        "    # Add value labels on bars\n",
        "    for bar, value in zip(bars1, scores_avg.values):\n",
        "        height = bar.get_height()\n",
        "        if not pd.isna(height):\n",
        "            ax1.text(bar.get_x() + bar.get_width()/2., height + 0.05,\n",
        "                    f'{height:.2f}', ha='center', va='bottom')\n",
        "\n",
        "    # 2. Key Terms Performance\n",
        "    ax2 = axes[1]\n",
        "    if len(plot_data) > 0:\n",
        "        term_scores = plot_data.groupby('Key Term Name')[score_columns].mean().mean(axis=1)\n",
        "        bars2 = ax2.bar(range(len(term_scores)), term_scores.values,\n",
        "                       color=['#FFD93D', '#6BCF7F', '#4D96FF'])\n",
        "        ax2.set_xlabel('Key Terms')\n",
        "        ax2.set_ylabel('Average Score')\n",
        "        ax2.set_title('Performance by Key Term')\n",
        "        ax2.set_xticks(range(len(term_scores)))\n",
        "        ax2.set_xticklabels(term_scores.index, rotation=45, ha='right')\n",
        "        ax2.set_ylim(0, 5)\n",
        "\n",
        "        # Add value labels\n",
        "        for bar, value in zip(bars2, term_scores.values):\n",
        "            height = bar.get_height()\n",
        "            if not pd.isna(height):\n",
        "                ax2.text(bar.get_x() + bar.get_width()/2., height + 0.05,\n",
        "                        f'{height:.2f}', ha='center', va='bottom')\n",
        "\n",
        "    # 3. Overall Performance Gauge\n",
        "    ax3 = axes[2]\n",
        "    overall_score = plot_data[score_columns].mean().mean()\n",
        "\n",
        "    # Create a simple gauge chart\n",
        "    theta = np.linspace(0, np.pi, 100)\n",
        "    r = np.ones_like(theta)\n",
        "\n",
        "    # Color based on score\n",
        "    if pd.notna(overall_score):\n",
        "        if overall_score >= 4:\n",
        "            color = '#2ECC71'  # Green\n",
        "            status = 'Excellent'\n",
        "        elif overall_score >= 3:\n",
        "            color = '#F39C12'  # Orange\n",
        "            status = 'Good'\n",
        "        elif overall_score >= 2:\n",
        "            color = '#E74C3C'  # Red\n",
        "            status = 'Fair'\n",
        "        else:\n",
        "            color = '#95A5A6'  # Gray\n",
        "            status = 'Poor'\n",
        "\n",
        "        # Plot gauge\n",
        "        ax3.fill_between(theta, 0, r, alpha=0.3, color='lightgray')\n",
        "        gauge_theta = np.linspace(0, np.pi * (overall_score/5), 50)\n",
        "        gauge_r = np.ones_like(gauge_theta)\n",
        "        ax3.fill_between(gauge_theta, 0, gauge_r, alpha=0.8, color=color)\n",
        "\n",
        "        ax3.set_ylim(0, 1)\n",
        "        ax3.set_xlim(0, np.pi)\n",
        "        ax3.set_title('Overall Performance')\n",
        "        ax3.text(np.pi/2, 0.5, f'{overall_score:.2f}\\n{status}',\n",
        "                ha='center', va='center', fontsize=14, fontweight='bold')\n",
        "        ax3.set_xticks([])\n",
        "        ax3.set_yticks([])\n",
        "        ax3.spines['top'].set_visible(False)\n",
        "        ax3.spines['right'].set_visible(False)\n",
        "        ax3.spines['bottom'].set_visible(False)\n",
        "        ax3.spines['left'].set_visible(False)\n",
        "\n",
        "    plt.tight_layout()\n",
        "    return fig"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1xZeI90O2bEl"
      },
      "source": [
        "## Step 10 :  Gradio Interface – Legal Document Analyzer\n",
        "\n",
        "### Purpose\n",
        "\n",
        "This section defines the **Gradio user interface (UI)** that enables users to upload legal documents, extract key contractual terms, evaluate their quality using Azure LLM metrics, and visualize the results through an interactive dashboard.\n",
        "\n",
        "---\n",
        "\n",
        "### Function: `gradio_process_wrapper(file)`\n",
        "\n",
        "A wrapper function that connects the Gradio frontend input to the backend processing pipeline.\n",
        "\n",
        "#### Workflow:\n",
        "\n",
        "1. Receives a document upload (`file`), then calls `process_document(file)` to:\n",
        "   - Extract document text and key contractual terms.\n",
        "   - Run quality evaluation metrics on the extracted data.\n",
        "2. If evaluation results are available, invokes `create_dashboard()` to generate a comprehensive visualization of performance.\n",
        "3. Returns the following outputs to the UI:\n",
        "   - Status message indicating success or error.\n",
        "   - Table of extracted key terms with associated values and locations.\n",
        "   - Table of LLM evaluation scores per term.\n",
        "   - Matplotlib figure containing the evaluation dashboard."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k9i-Br9Ajpag"
      },
      "outputs": [],
      "source": [
        "# Gradio Interface\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "def gradio_process_wrapper(file):\n",
        "    \"\"\"Wrapper function for Gradio that processes document and returns dashboard\"\"\"\n",
        "\n",
        "    # Process the document\n",
        "    status, extracted_df, evaluation_df = process_document(file)\n",
        "\n",
        "    # Create dashboard if evaluation data exists\n",
        "    dashboard_plot = None\n",
        "    if evaluation_df is not None and not evaluation_df.empty:\n",
        "        dashboard_plot = create_dashboard(evaluation_df)\n",
        "\n",
        "    return status, extracted_df, evaluation_df, dashboard_plot\n",
        "\n",
        "# Create Gradio interface\n",
        "with gr.Blocks(title=\"Legal Document Analyzer\", theme=gr.themes.Soft()) as demo:\n",
        "    gr.Markdown(\n",
        "        \"\"\"\n",
        "        # 📋 Legal Document Analyzer\n",
        "\n",
        "        Upload a legal document (PDF, DOCX, TXT, CSV) and extract key terms with AI evaluation.\n",
        "\n",
        "        **Key Terms Extracted:**\n",
        "        - Product Name\n",
        "        - Limitation of Liability In Months\n",
        "        - Governing Law\n",
        "        \"\"\"\n",
        "    )\n",
        "\n",
        "    with gr.Row():\n",
        "        with gr.Column(scale=1):\n",
        "            # Upload section\n",
        "            gr.Markdown(\"## 📤 Upload Document\")\n",
        "            file_input = gr.File(\n",
        "                label=\"Choose Document\",\n",
        "                file_types=[\".pdf\", \".docx\", \".doc\", \".txt\", \".csv\"],\n",
        "                type=\"filepath\"\n",
        "            )\n",
        "\n",
        "            process_btn = gr.Button(\n",
        "                \"🚀 Start Evaluating\",\n",
        "                variant=\"primary\",\n",
        "                size=\"lg\"\n",
        "            )\n",
        "\n",
        "            # Status output\n",
        "            status_output = gr.Textbox(\n",
        "                label=\"Status\",\n",
        "                interactive=False,\n",
        "                placeholder=\"Upload a document and click 'Start Evaluating'...\"\n",
        "            )\n",
        "\n",
        "        with gr.Column(scale=2):\n",
        "            # Results section\n",
        "            gr.Markdown(\"## 📊 Evaluation Dashboard\")\n",
        "            dashboard_plot = gr.Plot(label=\"Performance Dashboard\")\n",
        "\n",
        "    with gr.Row():\n",
        "        with gr.Column():\n",
        "            gr.Markdown(\"## 📝 Extracted Information\")\n",
        "            extracted_table = gr.Dataframe(\n",
        "                label=\"Key Terms Extracted\",\n",
        "                interactive=False,\n",
        "                wrap=True\n",
        "            )\n",
        "\n",
        "        with gr.Column():\n",
        "            gr.Markdown(\"## 🎯 Evaluation Scores\")\n",
        "            evaluation_table = gr.Dataframe(\n",
        "                label=\"AI Evaluation Results\",\n",
        "                interactive=False,\n",
        "                wrap=True\n",
        "            )\n",
        "\n",
        "    # Event handlers\n",
        "    process_btn.click(\n",
        "        fn=gradio_process_wrapper,\n",
        "        inputs=[file_input],\n",
        "        outputs=[status_output, extracted_table, evaluation_table, dashboard_plot],\n",
        "        show_progress=True\n",
        "    )\n",
        "\n",
        "    # Example section\n",
        "    gr.Markdown(\n",
        "        \"\"\"\n",
        "        ## 📋 Evaluation Metrics Explained\n",
        "\n",
        "        - **Groundedness**: How well the response is supported by the context\n",
        "        - **Coherence**: How logical and well-structured the response is\n",
        "        - **Relevance**: How relevant the response is to the query\n",
        "        - **Fluency**: How natural and well-written the response is\n",
        "\n",
        "        *Scores range from 1-5, with 5 being the best.*\n",
        "        \"\"\"\n",
        "    )\n",
        "\n",
        "# Launch the interface\n",
        "if __name__ == \"__main__\":\n",
        "    demo.launch(debug=True, share=True)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}